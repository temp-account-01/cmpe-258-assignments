{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajviishah/CMPE-258/blob/main/Assignment%206/CMPE_258_Assignment_6_Part_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1 - supervised contrastive learning - Using new loss**\n",
        "\n",
        "Write a colab to demonstrate supervised contrastive learning loss based supervised classification versus regular softmax based one"
      ],
      "metadata": {
        "id": "V-jsMmw-Ys7b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMG1etpdR4rX",
        "outputId": "996f50c5-c7b2-4376-91cd-735b32d908c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 16.4 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████                            | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 194 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 204 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 225 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 235 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████                         | 245 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 256 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 266 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 276 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 286 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 296 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 307 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 317 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 327 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 337 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 358 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 368 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 378 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 389 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 399 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 409 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 419 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 430 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 440 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 450 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 460 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 471 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 481 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 491 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 501 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 512 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 522 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 532 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 542 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 552 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 563 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 573 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 583 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 593 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 604 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 614 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 624 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 634 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 645 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 655 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 665 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 675 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 686 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 696 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 706 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 716 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 727 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 737 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 747 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 757 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 768 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 778 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 788 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 798 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 808 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 819 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 829 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 839 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 849 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 860 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 870 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 880 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 890 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 901 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 911 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 921 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 931 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 942 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 952 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 962 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 972 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 983 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 993 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.0 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.0 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.0 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.0 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.16.1\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRIOFpj1J4So"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmyESKFpJ4Sq"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BWdqLqgJ4Sq",
        "outputId": "2b701b93-e882-4f14-8878-4f3524415fdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 3s 0us/step\n",
            "170508288/170498071 [==============================] - 3s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
            "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
          ]
        }
      ],
      "source": [
        "num_classes = 10\n",
        "input_shape = (32, 32, 3)\n",
        "\n",
        "# Load the train and test data splits\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Display shapes of train and test datasets\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJLIxKG0J4Sr"
      },
      "source": [
        "## Using image data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Us8lWaTpJ4Sr"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.02),\n",
        "        layers.RandomWidth(0.2),\n",
        "        layers.RandomHeight(0.2),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Setting the state of the normalization layer.\n",
        "data_augmentation.layers[0].adapt(x_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi61bSZzJ4St"
      },
      "source": [
        "## Build the encoder model\n",
        "\n",
        "The encoder model takes the image as input and turns it into a 2048-dimensional\n",
        "feature vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arg89FBZJ4St",
        "outputId": "342418c9-1a08-4cfb-aba8-fda03771a0b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"cifar10-encoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " sequential (Sequential)     (None, 32, 32, 3)         7         \n",
            "                                                                 \n",
            " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,564,807\n",
            "Trainable params: 23,519,360\n",
            "Non-trainable params: 45,447\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def create_encoder():\n",
        "    resnet = keras.applications.ResNet50V2(\n",
        "        include_top=False, weights=None, input_shape=input_shape, pooling=\"avg\"\n",
        "    )\n",
        "\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    augmented = data_augmentation(inputs)\n",
        "    outputs = resnet(augmented)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-encoder\")\n",
        "    return model\n",
        "\n",
        "\n",
        "encoder = create_encoder()\n",
        "encoder.summary()\n",
        "\n",
        "learning_rate = 0.001\n",
        "batch_size = 265\n",
        "hidden_units = 512\n",
        "projection_units = 128\n",
        "num_epochs = 50\n",
        "dropout_rate = 0.5\n",
        "temperature = 0.05"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyaTDQDmJ4Su"
      },
      "source": [
        "## Build the classification model\n",
        "\n",
        "The classification model adds a fully-connected layer on top of the encoder,\n",
        "plus a softmax layer with the target classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9G9lJ6xJ4Sv"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_classifier(encoder, trainable=True):\n",
        "\n",
        "    for layer in encoder.layers:\n",
        "        layer.trainable = trainable\n",
        "\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    features = encoder(inputs)\n",
        "    features = layers.Dropout(dropout_rate)(features)\n",
        "    features = layers.Dense(hidden_units, activation=\"relu\")(features)\n",
        "    features = layers.Dropout(dropout_rate)(features)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(features)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-classifier\")\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    )\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdogEAesJ4Sv"
      },
      "source": [
        "## Experiment 1: Train the baseline classification model\n",
        "\n",
        "In this experiment, a baseline classifier is trained as usual, i.e., the\n",
        "encoder and the classifier parts are trained together as a single model\n",
        "to minimize the crossentropy loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Usvk2iCjJ4Sw",
        "outputId": "26ca34cd-ccb5-43fe-c859-ae9d95918535"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"cifar10-classifier\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " cifar10-encoder (Functional  (None, 2048)             23564807  \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               1049088   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24,619,025\n",
            "Trainable params: 24,573,578\n",
            "Non-trainable params: 45,447\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "189/189 [==============================] - 4793s 25s/step - loss: 1.9693 - sparse_categorical_accuracy: 0.2753\n",
            "Epoch 2/50\n",
            "189/189 [==============================] - 4935s 26s/step - loss: 1.5392 - sparse_categorical_accuracy: 0.4433\n",
            "Epoch 3/50\n",
            "104/189 [===============>..............] - ETA: 39:10 - loss: 1.4244 - sparse_categorical_accuracy: 0.4885"
          ]
        }
      ],
      "source": [
        "encoder = create_encoder()\n",
        "classifier = create_classifier(encoder)\n",
        "classifier.summary()\n",
        "\n",
        "history = classifier.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs)\n",
        "\n",
        "accuracy = classifier.evaluate(x_test, y_test)[1]\n",
        "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M0VTA9EJ4Sw"
      },
      "source": [
        "## Experiment 2: Use supervised contrastive learning\n",
        "\n",
        "### 1. Supervised contrastive learning loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEXH_zLjJ4Sx"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SupervisedContrastiveLoss(keras.losses.Loss):\n",
        "    def __init__(self, temperature=1, name=None):\n",
        "        super(SupervisedContrastiveLoss, self).__init__(name=name)\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
        "        # Normalize feature vectors\n",
        "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
        "        # Compute logits\n",
        "        logits = tf.divide(\n",
        "            tf.matmul(\n",
        "                feature_vectors_normalized, tf.transpose(feature_vectors_normalized)\n",
        "            ),\n",
        "            self.temperature,\n",
        "        )\n",
        "        return tfa.losses.npairs_loss(tf.squeeze(labels), logits)\n",
        "\n",
        "\n",
        "def add_projection_head(encoder):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    features = encoder(inputs)\n",
        "    outputs = layers.Dense(projection_units, activation=\"relu\")(features)\n",
        "    model = keras.Model(\n",
        "        inputs=inputs, outputs=outputs, name=\"cifar-encoder_with_projection-head\"\n",
        "    )\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcvlOCjxJ4Sx"
      },
      "source": [
        "### 2. Pretrain the encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kB7VxOzBJ4Sx"
      },
      "outputs": [],
      "source": [
        "encoder = create_encoder()\n",
        "\n",
        "encoder_with_projection_head = add_projection_head(encoder)\n",
        "encoder_with_projection_head.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate),\n",
        "    loss=SupervisedContrastiveLoss(temperature),\n",
        ")\n",
        "\n",
        "encoder_with_projection_head.summary()\n",
        "\n",
        "history = encoder_with_projection_head.fit(\n",
        "    x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-S_UGvzJ4Sy"
      },
      "source": [
        "### 3. Train the classifier with the frozen encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUWIrPijJ4Sy"
      },
      "outputs": [],
      "source": [
        "classifier = create_classifier(encoder, trainable=False)\n",
        "\n",
        "history = classifier.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs)\n",
        "\n",
        "accuracy = classifier.evaluate(x_test, y_test)[1]\n",
        "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KowxYCtbJ4Sy"
      },
      "source": [
        "We get to an improved test accuracy."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CMPE 258 - Assignment 6 Part 1",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}